# @package _global_
# Distributed CFM training configuration for Hydra


# Training mode
training_mode: cfm

# Model configuration
model:
  # CFM model (DiT) configuration
  dim: 2048              # DiT model dimension
  num_layers: 12        # Number of transformer layers
  expansion_factor: 4   # FFN expansion factor
  max_seq_len: 128
  
  # Pre-trained LVAE path (required for CFM)
  lvae_model_path: "/path/to/trained/lvae/checkpoint"
  
  # These will be loaded from the LVAE checkpoint:
  # vocab_size, d_model, latent_dim, num_latents, dim_head, max_seq_len, tokenizer_name

# Training configuration
train_num_steps: 50000
train_bs: 256
eval_bs: 256
eval_every: 500
grad_accumulate: 1
compile_model: true
resume_from: null
results_folder: null

# Data configuration
train_bin_pattern: "/p/vast1/kirchenb/.cache/ldlm/datasets/fineweb100B/fineweb_train_*.bin"
val_bin_pattern: "/p/vast1/kirchenb/.cache/ldlm/datasets/fineweb100B/fineweb_val_*.bin"
total_tokens: 100000000000  # 100B tokens

# Optimization configuration
learning_rate: 1e-4
adam_betas: [0.9, 0.95]
adam_weight_decay: 0.1
adam_eps: 1e-8
muon_lr: 0.02
muon_momentum: 0.95
lr_warmup_steps: 100

# CFM-specific configuration
cfm_gen_steps: 50          # Steps for generation during eval

# Logging configuration
seed: 42
log_step_interval: 1
output_dir: "outputs"
wandb_name: "cfm-distributed"
save_checkpoint: true

# Cluster shenanigans
per_process_vram_ratio: 0.8  # eg. 0.8, For APUs like MI300A and GH200

# Resume configuration (optional)
# resume_from: "outputs/model_best.pt"

# Hydra configuration
hydra:
  run:
    dir: ${output_dir}/${now:%Y-%m-%d}/${now:%H-%M-%S}
  job:
    chdir: false 
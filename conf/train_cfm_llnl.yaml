defaults:
  - _self_

# General settings
general:
  seed: 412
  wandb_name: "cfm_diffusion_model"
  resume_training: False
  resume_dir: null
  checkpoint_path: null  # Path to resume from external checkpoint directory (overrides resume_dir)
  eval: False

# Data configuration
data:
  # BIN FILE APPROACH (recommended for performance with large datasets)
  # Remove the comment (#) from the lines below to use .bin files:
  train_bin_pattern: "/p/vast1/kirchenb/.cache/ldlm/datasets/fineweb100B/fineweb_train_*.bin"
  val_bin_pattern: "/p/vast1/kirchenb/.cache/ldlm/datasets/fineweb100B/fineweb_val_*.bin"
  total_tokens: 100000000000

  # TRADITIONAL DATASET LOADING (default approach)
  # Comment out these lines if using bin files above:
  # dataset_name: "fineweb-edu_10b"
  # num_samples: null
  # precomputed_latents are not supported with the new VAE, so these are commented out
  # use_precomputed_latents: False
  # precomputed_latent_path: "./precomputed_latents/"

# Model configuration
model:
  # Path to the directory containing the trained VAE (e.g., .../916ca5193e27e9b884dc09d5bf66fe33/)
  latent_model_path: null
  # Diffusion model (DiT) parameters
  dim: 2048
  num_layers: 24
  expansion_factor: 4
  # BIN FILE APPROACH (recommended for performance with large datasets)
  # Remove the comment (#) from the lines below to use .bin files:
  tokenizer_name: "gpt2"
  # TRADITIONAL DATASET LOADING (default approach)
  # Comment out these lines if using bin files above:
  # tokenizer_name: "meta-llama/Llama-3.2-1B"

# Training parameters
training:
  train_bs: 64
  eval_bs: 64
  train_num_steps: 80000
  grad_accumulate: 16
  mixed_precision: "bf16"
  save_and_sample_every: 1000
  eval_every: 1000
  ema_decay: 0.995
  ema_update_every: 10

  # Optimizer settings
  optimizer:
    learning_rate: 1e-5
    lr_schedule: "linear"
    lr_warmup_steps: 100
    adam_betas: [0.9, 0.99]
    adam_weight_decay: 0.01

# Evaluation settings (used when general.eval=True)
eval:
  path: null # Path to the checkpoint directory for evaluation
  num_gen_samples: 5
  gen_steps: 100
  gen_batch_size: 5
  gen_max_length: 256 
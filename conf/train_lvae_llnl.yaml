defaults:
  - _self_

# General
seed: 412
wandb_name: "scratch_latent_vae"
resume_training: False
resume_dir: null
eval: False
save_dir: "saved_latent_models"

# Data
data:
  dataset_name: "fineweb-edu_10b"
  num_samples: null
  train_bin_pattern: "/scratch/gpfs/ashwinee/new/modded-nanogpt/data/fineweb100B/fineweb_train_*.bin"
  val_bin_pattern: "/scratch/gpfs/ashwinee/new/modded-nanogpt/data/fineweb100B/fineweb_val_*.bin"
  total_tokens: 100000000000

# Model
model:
  max_seq_len: 128
  d_model: 768
  latent_dim: 1536
  num_latents: 32
  dim_head: 128
  num_layers: 8
  latent_model_path: null  # Path to resume from external checkpoint
  tokenizer_name: "gpt2"
# Training
training:
  train_bs: 48
  eval_bs: 8
  train_num_steps: 10000
  eval_every: 1000
  grad_accumulate: 16
  mixed_precision: "bf16"
  kld_weight: 1e-3
  # Optimizer
  optimizer:
    learning_rate: 1e-4
    lr_schedule: "linear"
    lr_warmup_steps: 50
    adam_betas: [0.9, 0.99]
    adam_weight_decay: 0.01
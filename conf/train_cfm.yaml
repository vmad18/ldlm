defaults:
  - override hydra/launcher: my_cluster
  - _self_

# General settings
general:
  seed: 412
  wandb_name: "cfm_diffusion_model"
  resume_training: False
  resume_dir: null
  checkpoint_path: null  # Path to resume from external checkpoint directory (overrides resume_dir)
  eval: False

# Data configuration
data:
  # BIN FILE APPROACH (recommended for performance with large datasets)
  # Remove the comment (#) from the lines below to use .bin files:
  # train_bin_pattern: "data/train_*.bin"  # glob pattern for training .bin files
  # val_bin_pattern: "data/val_*.bin"      # glob pattern for validation .bin files (optional)
  # total_tokens: 100000000000  # Total tokens in dataset (100B) - used for epoch calculation with bin files
  
  # TRADITIONAL DATASET LOADING (default approach)
  # Comment out these lines if using bin files above:
  dataset_name: "fineweb-edu_10b"
  shard_size: null

# Model configuration
model:
  # Path to the directory containing the trained VAE (e.g., .../916ca5193e27e9b884dc09d5bf66fe33/)
  latent_model_path: "/scratch/gpfs/ashwinee/new/ldlm/outputs/2025-07-01/02-30-25/143452a169c694e91cedd27efe948d04"
  # Diffusion model (DiT) parameters
  dim: 2048
  num_layers: 24
  expansion_factor: 4

# Training parameters
training:
  train_bs: 64
  eval_bs: 64
  train_num_steps: 80000
  grad_accumulate: 16
  mixed_precision: "bf16"
  save_and_sample_every: 1000
  eval_every: 1000
  ema_decay: 0.995
  ema_update_every: 10

  # Optimizer settings
  optimizer:
    learning_rate: 1e-5
    lr_schedule: "linear"
    lr_warmup_steps: 100
    adam_betas: [0.9, 0.99]
    adam_weight_decay: 0.01

# Evaluation settings (used when general.eval=True)
eval:
  path: null # Path to the checkpoint directory for evaluation
  num_gen_samples: 5
  gen_steps: 100
  gen_batch_size: 5
  gen_max_length: 256 
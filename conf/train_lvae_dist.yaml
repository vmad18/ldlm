# @package _global_
# Distributed LVAE training configuration for Hydra

defaults:
  - override hydra/launcher: my_cluster
  - _self_

# Model configuration
model:
  vocab_size: 50257
  d_model: 768
  latent_dim: 2048
  num_latents: 1
  dim_head: 128
  max_seq_len: 128
  num_layers: 12
  tokenizer_name: "gpt2"

# Training configuration
train_num_steps: 100000
train_bs: 64
eval_bs: 64
eval_every: 50
grad_accumulate: 2
compile_model: true
resume_from: null
results_folder: null

# Data configuration
train_bin_pattern: "/scratch/gpfs/ashwinee/new/modded-nanogpt/data/fineweb100B/fineweb_train_*.bin"
val_bin_pattern: "/scratch/gpfs/ashwinee/new/modded-nanogpt/data/fineweb100B/fineweb_val_*.bin"
total_tokens: 100000000000  # 100B tokens

# Optimization configuration
learning_rate: 1e-4
adam_betas: [0.9, 0.95]  # Changed from separate adam_beta1/adam_beta2 to tuple
adam_weight_decay: 0.1
adam_eps: 1e-8
muon_lr: 0.02
muon_momentum: 0.95

# VAE configuration
kld_weight: 1e-4
kld_annealing_steps: 2000

# Logging configuration
seed: 42
log_step_interval: 1
output_dir: "outputs"
wandb_name: "lvae-distributed"
save_checkpoint: true

# Cluster shenanigans
per_process_vram_ratio: null  # eg. 0.8, For APUs like MI300A and GH200

# Resume configuration (optional)
# resume_from: "outputs/model_best.pt"

# Hydra configuration
hydra:
  run:
    dir: ${output_dir}/${now:%Y-%m-%d}/${now:%H-%M-%S}
  job:
    chdir: false 